{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5068cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Day 4: NLP Basics\n",
      "Today we'll understand how AI 'reads' and processes text\n"
     ]
    }
   ],
   "source": [
    "# Day 4: NLP Basics - Understanding the Foundation of AI Text Processing\n",
    "# Goal: Learn tokenization, embeddings, transformers + build our first summarizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"🚀 Day 4: NLP Basics\")\n",
    "print(\"Today we'll understand how AI 'reads' and processes text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516562d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple tokenization (by spaces):\n",
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']\n",
      "Number of tokens: 9\n"
     ]
    }
   ],
   "source": [
    "# CONCEPT 1: TOKENIZATION\n",
    "# Breaking text into smaller, manageable pieces\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Simple word tokenization (splitting by spaces)\n",
    "simple_tokens = text.split()\n",
    "print(\"Simple tokenization (by spaces):\")\n",
    "print(simple_tokens)\n",
    "print(f\"Number of tokens: {len(simple_tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45bd574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI tokenization:\n",
      "['The', 'Ġquick', 'Ġbrown', 'Ġfox', 'Ġjumps', 'Ġover', 'Ġthe', 'Ġlazy', 'Ġdog', '.']\n",
      "Number of tokens: 10\n",
      "\n",
      "What the AI actually processes (numbers):\n",
      "[0, 133, 2119, 6219, 23602, 13855, 81, 5, 22414, 2335, 4, 2]\n",
      "Number of token IDs: 12\n"
     ]
    }
   ],
   "source": [
    "# How AI systems ACTUALLY tokenize text\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the SAME lightweight tokenizer we'll use throughout\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-12-6\")\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Real AI tokenization\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"AI tokenization:\")\n",
    "print(tokens)\n",
    "print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Convert tokens to numbers (what the AI actually sees)\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"\\nWhat the AI actually processes (numbers):\")\n",
    "print(token_ids)\n",
    "print(f\"Number of token IDs: {len(token_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49700cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer output structure:\n",
      "Token IDs shape: torch.Size([1, 12])\n",
      "First few token IDs: tensor([    0,   133,  2119,  6219, 23602, 13855,    81,     5, 22414,  2335])\n",
      "\n",
      "What these numbers mean: ['<s>', 'The', ' quick', ' brown', ' fox', ' jumps', ' over', ' the', ' lazy', ' dog']\n"
     ]
    }
   ],
   "source": [
    "# CONCEPT 2: EMBEDDINGS\n",
    "# Converting tokens into vectors that capture MEANING\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Get embeddings for our text\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"Tokenizer output structure:\")\n",
    "print(f\"Token IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"First few token IDs: {inputs['input_ids'][0][:10]}\")\n",
    "\n",
    "# Let's see what tokens these numbers represent\n",
    "decoded_tokens = [tokenizer.decode([token_id]) for token_id in inputs['input_ids'][0][:10]]\n",
    "print(f\"\\nWhat these numbers mean: {decoded_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6659ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dog' becomes token: 16319\n",
      "'cat' becomes token: 8729\n",
      "'car' becomes token: 5901\n",
      "'puppy' becomes token: 642\n"
     ]
    }
   ],
   "source": [
    "# Let's test word relationships with a simple example\n",
    "test_words = [\"dog\", \"cat\", \"car\", \"puppy\"]\n",
    "\n",
    "for word in test_words:\n",
    "    # Tokenize each word\n",
    "    tokens = tokenizer.encode(word, return_tensors=\"pt\")\n",
    "    print(f\"'{word}' becomes token: {tokens[0][1].item()}\")  # Skip special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1177b473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 TRANSFORMERS - Using a lightweight model for learning\n",
      "\n",
      "✅ Loaded sshleifer/distilbart-cnn-12-6\n",
      "This is a 'distilled' version - smaller but still powerful!\n",
      "Perfect for learning and development\n"
     ]
    }
   ],
   "source": [
    "# CONCEPT 3: TRANSFORMERS (Using a smaller, faster model)\n",
    "# Let's use a lightweight model that downloads quickly\n",
    "\n",
    "# Import both the tokenizer AND model classes\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "print(\"🤖 TRANSFORMERS - Using a lightweight model for learning\")\n",
    "\n",
    "# Using a smaller, faster model\n",
    "model_name = \"sshleifer/distilbart-cnn-12-6\"  # Much smaller version\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f\"\\n✅ Loaded {model_name}\")\n",
    "print(f\"This is a 'distilled' version - smaller but still powerful!\")\n",
    "print(\"Perfect for learning and development\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a41fe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Original text:\n",
      "Length: 662 characters\n",
      "\n",
      "The quarterly financial report shows that our company achieved record revenue of $2.5 million, \n",
      "representing a 35% increase from the previous quarter. Sales were particularly strong in the \n",
      "enterprise software division, which contributed 60% of total revenue. However, marketing \n",
      "expenses increased by 40% due to expanded digital advertising campaigns. The engineering \n",
      "team hired 15 new developers, increasing operational costs but positioning us for future \n",
      "product launches. Customer satisfaction scores improved to 4.2 out of 5.0, up from 3.8 \n",
      "last quarter. Looking forward, we expect continued growth but will need to optimize \n",
      "marketing spend efficiency.\n",
      "\n",
      "\n",
      "🤖 AI Summary:\n",
      "Length: 277 characters\n",
      "Compression ratio: 41.84%\n",
      " Sales were particularly strong in the enterprise software division, which contributed 60% of total revenue . Marketing expenses increased by 40% due to expanded digital advertising campaigns . Customer satisfaction scores improved to 4.2 out of 5.0, up from 3.8 last quarter .\n"
     ]
    }
   ],
   "source": [
    "# TIME TO MAKE OUR FIRST AI SUMMARY!\n",
    "\n",
    "# Sample text to summarize (like a business document)\n",
    "business_text = \"\"\"\n",
    "The quarterly financial report shows that our company achieved record revenue of $2.5 million, \n",
    "representing a 35% increase from the previous quarter. Sales were particularly strong in the \n",
    "enterprise software division, which contributed 60% of total revenue. However, marketing \n",
    "expenses increased by 40% due to expanded digital advertising campaigns. The engineering \n",
    "team hired 15 new developers, increasing operational costs but positioning us for future \n",
    "product launches. Customer satisfaction scores improved to 4.2 out of 5.0, up from 3.8 \n",
    "last quarter. Looking forward, we expect continued growth but will need to optimize \n",
    "marketing spend efficiency.\n",
    "\"\"\"\n",
    "\n",
    "print(\"📄 Original text:\")\n",
    "print(f\"Length: {len(business_text)} characters\")\n",
    "print(business_text)\n",
    "\n",
    "# Create the summary using our AI model\n",
    "inputs = tokenizer.encode(business_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "summary_ids = model.generate(inputs, max_length=100, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\n🤖 AI Summary:\")\n",
    "print(f\"Length: {len(summary)} characters\")\n",
    "print(f\"Compression ratio: {len(summary)/len(business_text):.2%}\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b079fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "📋 Document Type: Legal Contract\n",
      "Original length: 538 characters\n",
      "AI Summary:  The Software License Agreement (\"Agreement\") is entered into between TechCorp (\"Licensor\")  and Client Company (\"Licensee\") The Licensor grants Licensee a non-exclusive,                 non-transferable license to use the software for internal business operations only . The license fee is $50,000 annually, payable quarterly .\n",
      "Compression: 60.97%\n",
      "\n",
      "==================================================\n",
      "📋 Document Type: Technical Report\n",
      "Original length: 518 characters\n",
      "AI Summary:  The primary bottleneck is identified in the user authentication module, which processes 10,000 requests per minute during peak hours . Memory usage has reached 85% of available capacity .\n",
      "Compression: 36.29%\n"
     ]
    }
   ],
   "source": [
    "# Let's test different document types - this is the business value!\n",
    "\n",
    "test_documents = {\n",
    "    \"Legal Contract\": \"\"\"\n",
    "This Software License Agreement (\"Agreement\") is entered into between TechCorp (\"Licensor\") \n",
    "and Client Company (\"Licensee\"). The Licensor grants Licensee a non-exclusive, \n",
    "non-transferable license to use the software for internal business operations only. \n",
    "The license fee is $50,000 annually, payable quarterly. Licensee may not reverse engineer, \n",
    "modify, or redistribute the software. This Agreement terminates automatically if Licensee \n",
    "breaches any terms. Licensor provides no warranty and limits liability to the license fee paid.\n",
    "\"\"\",\n",
    "    \n",
    "    \"Technical Report\": \"\"\"\n",
    "The system performance analysis reveals that database query response times have increased \n",
    "by 200% over the past month. The primary bottleneck is identified in the user authentication \n",
    "module, which processes 10,000 requests per minute during peak hours. Memory usage has \n",
    "reached 85% of available capacity. We recommend implementing query caching, upgrading \n",
    "to SSD storage, and adding two additional server instances. These improvements should \n",
    "reduce response times by 60% and handle projected 50% traffic growth.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "for doc_type, text in test_documents.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"📋 Document Type: {doc_type}\")\n",
    "    print(f\"Original length: {len(text)} characters\")\n",
    "    \n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=80, min_length=20, length_penalty=2.0, num_beams=4)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"AI Summary: {summary}\")\n",
    "    print(f\"Compression: {len(summary)/len(text):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84614435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💼 BUSINESS APPLICATIONS OF TEXT SUMMARIZATION\n",
      "\n",
      "1. LEGAL TEAMS:\n",
      "   - Summarize 50-page contracts in seconds\n",
      "   - Extract key terms, obligations, deadlines\n",
      "   - Save 2-3 hours per document review\n",
      "\n",
      "2. FINANCE TEAMS:\n",
      "   - Process quarterly reports quickly\n",
      "   - Extract KPIs and financial metrics\n",
      "   - Analyze competitor financial filings\n",
      "\n",
      "3. EXECUTIVES:\n",
      "   - Daily news summaries for industry updates\n",
      "   - Board meeting prep from lengthy documents\n",
      "   - Quick briefings from team reports\n",
      "\n",
      "💰 ROI EXAMPLE - Law Firm:\n",
      "Lawyer rate: $300/hour\n",
      "Time saved per document: 2.5 hours\n",
      "Documents per month: 40\n",
      "Monthly savings: $30,000.0\n",
      "Annual savings: $360,000.0\n",
      "That's $360K per year!\n"
     ]
    }
   ],
   "source": [
    "# BUSINESS VALUE - Why this matters for real companies\n",
    "\n",
    "print(\"💼 BUSINESS APPLICATIONS OF TEXT SUMMARIZATION\")\n",
    "print(\"\\n1. LEGAL TEAMS:\")\n",
    "print(\"   - Summarize 50-page contracts in seconds\")\n",
    "print(\"   - Extract key terms, obligations, deadlines\")\n",
    "print(\"   - Save 2-3 hours per document review\")\n",
    "\n",
    "print(\"\\n2. FINANCE TEAMS:\")\n",
    "print(\"   - Process quarterly reports quickly\") \n",
    "print(\"   - Extract KPIs and financial metrics\")\n",
    "print(\"   - Analyze competitor financial filings\")\n",
    "\n",
    "print(\"\\n3. EXECUTIVES:\")\n",
    "print(\"   - Daily news summaries for industry updates\")\n",
    "print(\"   - Board meeting prep from lengthy documents\")\n",
    "print(\"   - Quick briefings from team reports\")\n",
    "\n",
    "# Calculate potential ROI\n",
    "lawyer_hourly_rate = 300\n",
    "hours_saved_per_doc = 2.5\n",
    "docs_per_month = 40\n",
    "\n",
    "monthly_savings = lawyer_hourly_rate * hours_saved_per_doc * docs_per_month\n",
    "annual_savings = monthly_savings * 12\n",
    "\n",
    "print(f\"\\n💰 ROI EXAMPLE - Law Firm:\")\n",
    "print(f\"Lawyer rate: ${lawyer_hourly_rate}/hour\")\n",
    "print(f\"Time saved per document: {hours_saved_per_doc} hours\")\n",
    "print(f\"Documents per month: {docs_per_month}\")\n",
    "print(f\"Monthly savings: ${monthly_savings:,}\")\n",
    "print(f\"Annual savings: ${annual_savings:,}\")\n",
    "print(f\"That's ${annual_savings/1000:.0f}K per year!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f81a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎛️ TUNING YOUR SUMMARIZER\n",
      "\n",
      "Very Short Summary:\n",
      "Length: 248 chars\n",
      " Sales were particularly strong in the enterprise software division, which contributed 60% of total revenue . Marketing expenses increased by 40% due to expanded digital advertising campaigns . Customer satisfaction scores improved to 4.2 out of 5.\n",
      "----------------------------------------\n",
      "\n",
      "Medium Summary:\n",
      "Length: 277 chars\n",
      " Sales were particularly strong in the enterprise software division, which contributed 60% of total revenue . Marketing expenses increased by 40% due to expanded digital advertising campaigns . Customer satisfaction scores improved to 4.2 out of 5.0, up from 3.8 last quarter .\n",
      "----------------------------------------\n",
      "\n",
      "Detailed Summary:\n",
      "Length: 278 chars\n",
      " Sales were particularly strong in the enterprise software division, which contributed 60% of total revenue . Marketing expenses increased by 40% due to expanded digital advertising campaigns . Customer satisfaction scores improved to 4.2 out of 5.0, up from 3.8  last quarter .\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ADVANCED: Understanding how to tune summarization\n",
    "\n",
    "print(\"🎛️ TUNING YOUR SUMMARIZER\")\n",
    "\n",
    "sample_text = business_text  # Using our original business report\n",
    "\n",
    "# Test different parameters\n",
    "parameters = [\n",
    "    {\"max_length\": 50, \"min_length\": 20, \"name\": \"Very Short\"},\n",
    "    {\"max_length\": 100, \"min_length\": 30, \"name\": \"Medium\"},\n",
    "    {\"max_length\": 150, \"min_length\": 50, \"name\": \"Detailed\"},\n",
    "]\n",
    "\n",
    "for params in parameters:\n",
    "    inputs = tokenizer.encode(sample_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs, \n",
    "        max_length=params[\"max_length\"], \n",
    "        min_length=params[\"min_length\"], \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\n{params['name']} Summary:\")\n",
    "    print(f\"Length: {len(summary)} chars\")\n",
    "    print(summary)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da92fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 WEEK 1 MVP: SMART DOCUMENT SUMMARIZER\n",
      "\n",
      "Core Features we'll build:\n",
      "✅ Upload any text document\n",
      "✅ Choose summary type:\n",
      "   - Executive Brief (30-50 words)\n",
      "   - Standard Summary (80-120 words)\n",
      "   - Detailed Analysis (150+ words)\n",
      "✅ Industry-specific modes:\n",
      "   - Legal contracts\n",
      "   - Financial reports\n",
      "   - Technical documentation\n",
      "\n",
      "📊 Configuration for different user types:\n",
      "Executive: 20-50 words\n",
      "Standard: 40-120 words\n",
      "Detailed: 80-200 words\n"
     ]
    }
   ],
   "source": [
    "# YOUR WEEK 1 MVP - TEXT SUMMARIZER FEATURES\n",
    "\n",
    "print(\"🚀 WEEK 1 MVP: SMART DOCUMENT SUMMARIZER\")\n",
    "print(\"\\nCore Features we'll build:\")\n",
    "print(\"✅ Upload any text document\")\n",
    "print(\"✅ Choose summary type:\")\n",
    "print(\"   - Executive Brief (30-50 words)\")\n",
    "print(\"   - Standard Summary (80-120 words)\")\n",
    "print(\"   - Detailed Analysis (150+ words)\")\n",
    "print(\"✅ Industry-specific modes:\")\n",
    "print(\"   - Legal contracts\")\n",
    "print(\"   - Financial reports\") \n",
    "print(\"   - Technical documentation\")\n",
    "\n",
    "# Preview of what we're building\n",
    "document_types = {\n",
    "    \"executive\": {\"max_length\": 50, \"min_length\": 20},\n",
    "    \"standard\": {\"max_length\": 120, \"min_length\": 40},\n",
    "    \"detailed\": {\"max_length\": 200, \"min_length\": 80}\n",
    "}\n",
    "\n",
    "print(f\"\\n📊 Configuration for different user types:\")\n",
    "for user_type, config in document_types.items():\n",
    "    print(f\"{user_type.title()}: {config['min_length']}-{config['max_length']} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af8922cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DAY 4 ACHIEVEMENTS:\n",
      "🧠 Understood tokenization (how AI reads text)\n",
      "🧠 Understood embeddings (how AI represents meaning)\n",
      "🧠 Understood transformers (how AI processes and generates)\n",
      "🤖 Loaded and tested a real AI summarization model\n",
      "💼 Identified business applications and ROI\n",
      "🎯 Designed MVP features for different user types\n",
      "\n",
      "🔮 TOMORROW (Day 5): Build Summarizer Backend\n",
      "We'll wrap this AI model in a FastAPI web service:\n",
      "- Input: Raw text document\n",
      "- Output: JSON with summary + metadata\n",
      "- Ready for web/mobile apps to consume\n",
      "\n",
      "📈 PROGRESS: 4/60 days complete\n",
      "Next week you'll have a live, deployed text summarizer!\n",
      "\n",
      "Test function output:  Sales were particularly strong in the enterprise software division, which contributed 60% of total revenue . Marketing expenses increased by 40% due to expanded digital advertising campaigns . Customer satisfaction scores improved to 4.2 out of 5.0, up from 3.8  last quarter . The engineering team hired 15 new developers, increasing operational costs but positioning us for future product launches .\n"
     ]
    }
   ],
   "source": [
    "# DAY 4 COMPLETE! 🎉\n",
    "\n",
    "print(\"✅ DAY 4 ACHIEVEMENTS:\")\n",
    "print(\"🧠 Understood tokenization (how AI reads text)\")\n",
    "print(\"🧠 Understood embeddings (how AI represents meaning)\")  \n",
    "print(\"🧠 Understood transformers (how AI processes and generates)\")\n",
    "print(\"🤖 Loaded and tested a real AI summarization model\")\n",
    "print(\"💼 Identified business applications and ROI\")\n",
    "print(\"🎯 Designed MVP features for different user types\")\n",
    "\n",
    "print(\"\\n🔮 TOMORROW (Day 5): Build Summarizer Backend\")\n",
    "print(\"We'll wrap this AI model in a FastAPI web service:\")\n",
    "print(\"- Input: Raw text document\")\n",
    "print(\"- Output: JSON with summary + metadata\")\n",
    "print(\"- Ready for web/mobile apps to consume\")\n",
    "\n",
    "print(f\"\\n📈 PROGRESS: 4/60 days complete\")\n",
    "print(\"Next week you'll have a live, deployed text summarizer!\")\n",
    "\n",
    "# Save a test function we'll use tomorrow\n",
    "def create_summary(text, summary_type=\"standard\"):\n",
    "    \"\"\"\n",
    "    Our core summarization function - we'll use this in our API tomorrow!\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        \"executive\": {\"max_length\": 50, \"min_length\": 20},\n",
    "        \"standard\": {\"max_length\": 120, \"min_length\": 40}, \n",
    "        \"detailed\": {\"max_length\": 200, \"min_length\": 80}\n",
    "    }\n",
    "    \n",
    "    config = configs.get(summary_type, configs[\"standard\"])\n",
    "    \n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=config[\"max_length\"],\n",
    "        min_length=config[\"min_length\"], \n",
    "        length_penalty=2.0,\n",
    "        num_beams=4\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Test our function\n",
    "test_summary = create_summary(business_text, \"detailed\")\n",
    "print(f\"\\nTest function output: {test_summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
